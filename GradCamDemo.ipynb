{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0760c1-341e-4e6b-a347-f414d72e1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms.transforms import ToPILImage\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03666ab0-0deb-4546-ab1a-8deaacacb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Set the model to 'evaluation' mode, that means freeze the weights\n",
    "model.eval()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf51c88-15dc-42ef-94e3-5c8e71d267d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target layer we want to inspect\n",
    "target_layer = model.layer4[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a80c1-0538-4f77-88e6-c8000dbc7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "pic = cv2.imread(\"images/dog/cat.jpg\", 1)\n",
    "\n",
    "# BGR to RGB\n",
    "img = pic.copy()\n",
    "img = img[:,:,::-1]\n",
    "img = np.ascontiguousarray(img)\n",
    "\n",
    "# Convert to torch tensor\n",
    "t = transforms.Compose([transforms.ToTensor()])\n",
    "img = t(img)\n",
    "\n",
    "# Add batch dimension\n",
    "img = img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7997cab2-614c-4524-8f92-2f48f680f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_hook(module, input, output):\n",
    "    activation.append(output)\n",
    "    \n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    grad.append(grad_out[0])\n",
    "    \n",
    "# Add hooks to get the tensors\n",
    "target_layer.register_forward_hook(forward_hook)\n",
    "target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "grad = []\n",
    "activation = []\n",
    "\n",
    "# forward pass to get the activation\n",
    "out = model(img)\n",
    "print(\"TOP 5\", torch.topk(out, 5))\n",
    "\n",
    "# class for dog\n",
    "loss = out[0, 178]\n",
    "\n",
    "# class for cat\n",
    "#loss = out[0, 285]\n",
    "print(\"LOSS\", loss.item())\n",
    "\n",
    "# clear the gradients\n",
    "model.zero_grad()\n",
    "\n",
    "# backward pass to get the gradients\n",
    "loss.backward()\n",
    "\n",
    "# get the gradients and activations collected in the hook\n",
    "grads = grad[0].cpu().data.numpy().squeeze()\n",
    "fmap = activation[0].cpu().data.numpy().squeeze()\n",
    "\n",
    "print(fmap.shape)\n",
    "print(grads.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92852c2-ddb2-4ef5-b375-a78c3529e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"grads.shape\", grads.shape)\n",
    "tmp = grads.reshape([grads.shape[0], -1])\n",
    "                     \n",
    "# Get the mean value of the gradients of every featuremap\n",
    "weights = np.mean(tmp, axis=1)\n",
    "print(\"weights.shape\", weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe71fcf-327f-43e0-b062-bb570b8888db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = np.zeros(grads.shape[1:])\n",
    "for i,w  in enumerate(weights):\n",
    "    cam += w*fmap[i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef9ce5-72fd-4429-ab57-f3a6c1e7a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = np.zeros(grads.shape[1:])\n",
    "\n",
    "for i,w in enumerate(weights):\n",
    "    cam += w*fmap[i, :]\n",
    "    cam = (cam>0)*cam\n",
    "    cam = cam / cam.max() * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fd752-a0ab-4593-94cc-702caf8d65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PIC SHAPE\", pic.shape)\n",
    "npic = np.array(torchvision.transforms.ToPILImage()(pic).convert('RGB'))\n",
    "print(\"NPIC SHAPE\", npic.shape)\n",
    "\n",
    "cam = cv2.resize(cam, (npic.shape[1], npic.shape[0]))\n",
    "print(\"CAM SHAPE\", cam.shape)\n",
    "\n",
    "heatmap = cv2.applyColorMap(np.uint8(cam), cv2.COLORMAP_JET)\n",
    "cam_img = npic*0.3 + heatmap*0.7\n",
    "print(cam_img.shape)\n",
    "\n",
    "display(torchvision.transforms.ToPILImage()(np.uint8(cam_img[:, :, ::-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba1566-d3f2-45c1-8ba5-b215c36f462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd369711-77c7-4ea8-b650-83a2d0bdf9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = torchvision.models.ResNet50_Weights.DEFAULT.meta[\"categories\"]\n",
    "for i,c in enumerate(categories):\n",
    "    print(i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09564e64-f215-4b85-8ed3-d02578f1a2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
