{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1c168f-629f-4181-915f-c9e93880f960",
   "metadata": {},
   "source": [
    "# Deep Feature Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e594f2-d234-41c9-8a1c-5b8ca1a0d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "from utils.image_loader import get_image_from_url, get_image_from_fs\n",
    "\n",
    "from pytorch_grad_cam import DeepFeatureFactorization\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image, deprocess_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ae912-136a-4513-a653-d2b967153ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "model = torchvision.models.densenet201(pretrained=True)\n",
    "\n",
    "# Set the model to 'evaluation' mode, that means freeze the weights\n",
    "model.eval()\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47585542-934b-48ac-a332-af60454df0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(concept_scores, top_k=2):\n",
    "    \"\"\" Create a list with the image-net category names of the top scoring categories\"\"\"\n",
    "    imagenet_categories_url = \\\n",
    "        \"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\"\n",
    "    labels = eval(requests.get(imagenet_categories_url).text)\n",
    "    concept_categories = np.argsort(concept_scores, axis=1)[:, ::-1][:, :top_k]\n",
    "    concept_labels_topk = []\n",
    "    for concept_index in range(concept_categories.shape[0]):\n",
    "        categories = concept_categories[concept_index, :]    \n",
    "        concept_labels = []\n",
    "        for category in categories:\n",
    "            score = concept_scores[concept_index, category]\n",
    "            label = f\"{labels[category].split(',')[0]}:{score:.2f}\"\n",
    "            concept_labels.append(label)\n",
    "        concept_labels_topk.append(\"\\n\".join(concept_labels))\n",
    "    return concept_labels_topk\n",
    "\n",
    "def show_factorization_on_image(img: np.ndarray,\n",
    "                                explanations: np.ndarray,\n",
    "                                colors: list[np.ndarray] = None,\n",
    "                                image_weight: float = 0.5,\n",
    "                                concept_labels: list = None) -> np.ndarray:\n",
    "    \"\"\" Color code the different component heatmaps on top of the image.\n",
    "        Every component color code will be magnified according to the heatmap itensity\n",
    "        (by modifying the V channel in the HSV color space),\n",
    "        and optionally create a lagend that shows the labels.\n",
    "        Since different factorization component heatmaps can overlap in principle,\n",
    "        we need a strategy to decide how to deal with the overlaps.\n",
    "        This keeps the component that has a higher value in it's heatmap.\n",
    "        \n",
    "        Taken from https://github.com/jacobgil/pytorch-grad-cam/blob/2183a9cbc1bd5fc1d8e134b4f3318c3b6db5671f/pytorch_grad_cam/utils/image.py#L83\n",
    "    :param img: The base image RGB format.\n",
    "    :param explanations: A tensor of shape num_componetns x height x width, with the component visualizations.\n",
    "    :param colors: List of R, G, B colors to be used for the components.\n",
    "                   If None, will use the gist_rainbow cmap as a default.\n",
    "    :param image_weight: The final result is image_weight * img + (1-image_weight) * visualization.\n",
    "    :concept_labels: A list of strings for every component. If this is paseed, a legend that shows\n",
    "                     the labels and their colors will be added to the image.\n",
    "    :returns: The visualized image.\n",
    "    \"\"\"\n",
    "    n_components = explanations.shape[0]\n",
    "    if colors is None:\n",
    "        # taken from https://github.com/edocollins/DFF/blob/master/utils.py\n",
    "        _cmap = plt.cm.get_cmap('gist_rainbow')\n",
    "        colors = [\n",
    "            np.array(\n",
    "                _cmap(i)) for i in np.arange(\n",
    "                0,\n",
    "                1,\n",
    "                1.0 /\n",
    "                n_components)]\n",
    "    concept_per_pixel = explanations.argmax(axis=0)\n",
    "    masks = []\n",
    "    for i in range(n_components):\n",
    "        mask = np.zeros(shape=(img.shape[0], img.shape[1], 3))\n",
    "        mask[:, :, :] = colors[i][:3]\n",
    "        explanation = explanations[i]\n",
    "        explanation[concept_per_pixel != i] = 0\n",
    "        mask = np.uint8(mask * 255)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_RGB2HSV)\n",
    "        mask[:, :, 2] = np.uint8(255 * explanation)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_HSV2RGB)\n",
    "        mask = np.float32(mask) / 255\n",
    "        masks.append(mask)\n",
    "\n",
    "    mask = np.sum(np.float32(masks), axis=0)\n",
    "    result = img * image_weight + mask * (1 - image_weight)\n",
    "    result = np.uint8(result * 255)\n",
    "\n",
    "    if concept_labels is not None:\n",
    "        px = 1 / plt.rcParams['figure.dpi']  # pixel in inches\n",
    "        fig = plt.figure(figsize=(result.shape[1] * px, result.shape[0] * px))\n",
    "        plt.rcParams['legend.fontsize'] = int(\n",
    "            14 * result.shape[0] / 256 / max(1, n_components / 6))\n",
    "        lw = 5 * result.shape[0] / 256\n",
    "        lines = [Line2D([0], [0], color=colors[i], lw=lw)\n",
    "                 for i in range(n_components)]\n",
    "        plt.legend(lines,\n",
    "                   concept_labels,\n",
    "                   mode=\"expand\",\n",
    "                   fancybox=True,\n",
    "                   shadow=True)\n",
    "\n",
    "        plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "        plt.axis('off')\n",
    "        fig.canvas.draw()\n",
    "        data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        plt.close(fig=fig)\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        data = cv2.resize(data, (result.shape[1], result.shape[0]))\n",
    "        result = np.hstack((result, data))\n",
    "    return result\n",
    "\n",
    "def dff(model, target_layer, computation_on_concepts, input_tensor, n_components):\n",
    "    dff_model = DeepFeatureFactorization(\n",
    "        model=model,\n",
    "        target_layer=target_layer, \n",
    "        computation_on_concepts=computation_on_concepts\n",
    "    )\n",
    "    if computation_on_concepts:\n",
    "        concepts, batch_explanations, concept_outputs = dff_model(input_tensor, n_components)\n",
    "    else:\n",
    "        concepts, batch_explanations = dff_model(input_tensor, n_components)\n",
    "        concept_outputs = None\n",
    "    return concepts, batch_explanations, concept_outputs\n",
    "        \n",
    "\n",
    "def visualize_image(\n",
    "    concepts,\n",
    "    batch_explanation,\n",
    "    concept_outputs,\n",
    "    rgb_image_float,\n",
    "    image_weight=0.3\n",
    "):  \n",
    "    if concept_outputs is not None:\n",
    "        concept_outputs = torch.softmax(\n",
    "            torch.from_numpy(concept_outputs),\n",
    "            axis=-1\n",
    "        ).numpy()    \n",
    "    \n",
    "    visualization = show_factorization_on_image(rgb_image_float, \n",
    "                                                batch_explanation,\n",
    "                                                image_weight=image_weight,\n",
    "                                                concept_labels=None)\n",
    "        \n",
    "    return visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f7d94-8123-4aa5-a417-a38c9d911472",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "rgb_img_floats = []\n",
    "input_tensors = []\n",
    "base_dir = \"images\"\n",
    "dir_name = \"containers\"\n",
    "n_components=2\n",
    "max_images=7\n",
    "resize=(395, 395)\n",
    "\n",
    "for i, filename in enumerate(os.listdir(base_dir + \"/\" + dir_name)):\n",
    "    if i >= max_images:\n",
    "        break\n",
    "    print(filename)\n",
    "    img, rgb_img_float, input_tensor = get_image_from_fs(\n",
    "        base_dir + \"/\" + dir_name + \"/\" + filename,\n",
    "        resize=resize,\n",
    "    )\n",
    "    images.append(img)\n",
    "    rgb_img_floats.append(rgb_img_float)\n",
    "    input_tensors.append(input_tensor)\n",
    "    \n",
    "input_tensor = torch.vstack(input_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bceafd-1466-4f89-a025-259e5fb5cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts, batch_explanations, concept_outputs = dff(\n",
    "    model=model,\n",
    "    target_layer=model.features.denseblock4,\n",
    "    computation_on_concepts=None,#model.classifier,\n",
    "    input_tensor=input_tensor,\n",
    "    n_components=n_components\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f1cbe-2053-4f16-a67d-f8b71a3b61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_data = {\n",
    "    \"concepts\": concepts,\n",
    "    \"batch_explanations\": batch_explanations,\n",
    "    \"concept_outputs\": concept_outputs,\n",
    "}\n",
    "\n",
    "np.savez_compressed(f\"dff_{dir_name}_concepts_{n_components}.npz\", concepts)\n",
    "np.savez_compressed(f\"dff_{dir_name}_batch_explanations_{n_components}.npz\", batch_explanations)\n",
    "np.savez_compressed(f\"dff_{dir_name}_concept_outputs_{n_components}.npz\", concept_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b0b77-3b4f-4164-8111-a6a355878e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "visualizations = []\n",
    "\n",
    "for i in range(len(batch_explanations)):\n",
    "    img, visualization = visualize_image(\n",
    "        concepts=concepts,\n",
    "        batch_explanation=batch_explanations[i],\n",
    "        concept_outputs=concept_outputs,\n",
    "        rgb_image_float=rgb_img_floats[i],\n",
    "        image_weight=0.3,\n",
    "    )\n",
    "    imgs.append(img)\n",
    "    visualizations.append(visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84abb83-5d66-4ebd-9175-49ac743a2ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(len(batch_explanations), 2, figsize=(8,64))\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "    #ax[i, 0].imshow(images[i])\n",
    "    #ax[i, 1].imshow(Image.fromarray(visualizations[i]))\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(1, 2, figsize=(8,8))\n",
    "    ax2[0].imshow(images[i])\n",
    "    ax2[1].imshow(Image.fromarray(visualizations[i]))\n",
    "    fig2.savefig(f\"visualizations/{dir_name}/{i}\" + \".jpg\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16b433-90e9-4e8a-8b0b-48afc60f6dcb",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536cd67-57fc-4b19-b2d4-51501165800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2, mode='deg'):\n",
    "    # Compute the dot product of the vectors\n",
    "    dot_product = np.dot(v1, v2)\n",
    "\n",
    "    # Compute the magnitudes of the vectors\n",
    "    magnitude_v1 = np.linalg.norm(v1)\n",
    "    magnitude_v2 = np.linalg.norm(v2)\n",
    "\n",
    "    # Compute the cosine of the angle between the vectors\n",
    "    cosine_angle = dot_product / (magnitude_v1 * magnitude_v2)\n",
    "\n",
    "    # Compute the angle in radians\n",
    "    angle_radians = np.arccos(cosine_angle)\n",
    "\n",
    "    # Convert the angle to degrees\n",
    "    angle_degrees = np.degrees(angle_radians)\n",
    "    \n",
    "    if mode == 'deg':\n",
    "        return angle_degrees\n",
    "    if mode == 'rad':\n",
    "        return angle_radians\n",
    "    \n",
    "def concept_similarity_matrix(concepts):\n",
    "    dim = concepts.shape[1]\n",
    "    matrix = np.zeros((dim, dim))\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if i == j:\n",
    "                matrix[i, j] = 0.0\n",
    "            else:\n",
    "                dist = cosine_similarity(concepts[:, i], concepts[:, j])\n",
    "                matrix[i, j] = dist\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9888daf-6e09-4568-b9df-e6064efc8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = concept_similarity_matrix(concepts)\n",
    "sns.heatmap(m, annot=True, cmap='YlOrRd', cbar=False, linewidths=0.5, fmt=\".2g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5146b6-af4d-47c0-bb35-94921f973aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
